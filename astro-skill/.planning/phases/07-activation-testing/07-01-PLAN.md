---
phase: 07-activation-testing
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - .planning/phases/07-activation-testing/07-VALIDATION.md
autonomous: false

must_haves:
  truths:
    - "3 technical + 3 conversational Astro/Cloudflare prompts are documented with keyword overlap analysis against SKILL.md description"
    - "4 negative prompts (neighboring framework, astronomy, Cloudflare-only, generic web) are documented with non-activation analysis"
    - "All 11 reference files have at least one navigation test question with verified grep pattern"
    - "MCP boundary is tested: 2 prompts correctly identified as MCP-appropriate (answer NOT in skill files), 2 prompts correctly identified as skill-appropriate (answer IS in skill files)"
    - "All 102 grep patterns from Phase 05-02 still return exactly 1 line each"
  artifacts:
    - path: ".planning/phases/07-activation-testing/07-VALIDATION.md"
      provides: "Structured validation report with pass/fail tables for activation, navigation, and MCP tests"
      contains: "## Test 1: Activation Scenarios"
  key_links:
    - from: "07-VALIDATION.md activation prompts"
      to: "SKILL.md description field"
      via: "keyword matching verification"
      pattern: "description:"
    - from: "07-VALIDATION.md navigation tests"
      to: "references/*.md headings"
      via: "grep -n pattern execution"
      pattern: "grep -n"
---

<objective>
Create and execute the activation, navigation, and MCP validation tests for the Astro/Cloudflare skill.

Purpose: Verify that the skill's auto-activation keywords match natural prompts, that grep-based navigation hits the correct reference file sections, and that the MCP boundary logic is correctly documented. These are mechanical tests that can be verified by running grep commands and reading file contents -- no live Claude session needed.

Output: `.planning/phases/07-activation-testing/07-VALIDATION.md` with completed Test 1 (Activation), Test 2 (Navigation), and Test 3 (MCP Integration) sections.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-activation-testing/07-CONTEXT.md
@.planning/phases/07-activation-testing/07-RESEARCH.md
@.claude/skills/astro-cloudflare/SKILL.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create validation report with activation and navigation tests</name>
  <files>.planning/phases/07-activation-testing/07-VALIDATION.md</files>
  <action>
Create the validation report file with three test sections. For each section, design the test, execute verification, and record results inline.

**Test 1: Activation Scenarios (TEST-01)**

1. Read `SKILL.md` frontmatter description field (the 1017-char description)
2. Extract all activation keywords from the description (file patterns like *.astro, framework terms like Astro/Cloudflare, action verbs)
3. Design 6 positive activation prompts split into two explicit categories:

   **Technical prompts (3):**
   - T1: Component/islands domain (e.g., "create an Astro component with client:visible hydration")
   - T2: Cloudflare-platform domain (e.g., "configure wrangler for D1 binding in my Astro project")
   - T3: Build/deploy domain (e.g., "set up CI/CD for Astro on Cloudflare Workers")

   **Conversational prompts (3):**
   - C1: Troubleshooting (e.g., "my Astro site doesn't deploy to Cloudflare")
   - C2: Decision-seeking (e.g., "should I use SSR or SSG for my Astro blog on Workers?")
   - C3: Migration/upgrade (e.g., "I'm upgrading from Astro 4 and my content collections broke")

4. Design 4 negative activation prompts testing boundary cases:
   - N1: Neighboring framework (Next.js on Vercel)
   - N2: "Astro" in non-framework context (astronomy)
   - N3: Cloudflare without Astro (Hono on Workers)
   - N4: Generic web dev (pure CSS/HTML question)
5. For each prompt, verify keyword overlap with the description field. Document which keywords match (or don't match). Mark as PASS if positive prompts have keyword overlap, FAIL if they don't. Mark negative prompts as PASS if they lack sufficient keyword overlap.
6. Include a "Category" column in the results table (Technical / Conversational / Negative)

**Test 2: Reference Navigation (TEST-02)**

1. Read `SKILL.md` body to find the grep hints section
2. For each of the 11 reference files, design one domain-specific test question that is answerable ONLY from that reference file
3. For each question, identify the grep pattern from SKILL.md that should route to the correct file/section
4. Run each grep pattern against the actual reference file and verify it returns exactly 1 line targeting the right section
5. Additionally, run ALL 102 grep patterns from SKILL.md body and verify each returns exactly 1 line (regression test from Phase 05-02). Record count of passing/failing patterns.
6. Document results in a table: File | Test Question | Grep Pattern | Lines Returned | PASS/FAIL

**Test 3: MCP Integration (TEST-03)**

This test verifies the MCP boundary logic mechanically by checking whether answers exist in skill files. This is a content coverage test, not a behavioral test of actual `search_astro_docs` tool invocation. Behavioral MCP verification (does Claude actually call the tool?) is validated during the session resilience test in Plan 07-02, where the user observes real Claude behavior in a live session.

1. Read SKILL.md MCP integration section
2. Design 2 prompts that SHOULD trigger MCP usage (API detail questions not covered in skill: exhaustive config options, full API signatures)
3. Design 2 prompts that SHOULD use the skill instead of MCP (decision/anti-pattern questions covered in skill)
4. For each prompt, verify the boundary logic by checking whether the answer exists in skill files or not:
   - If answer IS in skill files: prompt should use skill (not MCP) -- grep skill files to confirm content exists
   - If answer is NOT in skill files: prompt should use MCP -- grep skill files to confirm content is absent
5. Pass criteria: All 4 prompts correctly categorized (2 MCP-appropriate confirmed absent from skill, 2 skill-appropriate confirmed present in skill)
6. Document results with reasoning in table: Prompt | Expected Route | Grep Evidence | Correctly Categorized? | P/F

**On mechanical failure (grep patterns returning wrong line counts, keywords not matching):** Fix the issue in the relevant file immediately, then re-verify. Document the fix in a "Fixes Applied" section.

**On behavioral concerns (e.g., boundary ambiguity, unclear MCP routing logic):** Document the concern in a "Notes for Review" subsection. These are reviewed at the checkpoint.

Format each test section as a markdown table with: test ID, category (where applicable), prompt/pattern, expected result, actual result, PASS/FAIL, and notes.
  </action>
  <verify>
- `grep -c "PASS\|FAIL" .planning/phases/07-activation-testing/07-VALIDATION.md` returns a number >= 20 (6 positive + 4 negative + 11 navigation + 4 MCP = 25 minimum test rows)
- `grep -c "FAIL" .planning/phases/07-activation-testing/07-VALIDATION.md` shows number of failures (ideally 0)
- File contains sections: "## Test 1", "## Test 2", "## Test 3"
- `grep -c "Technical\|Conversational\|Negative" .planning/phases/07-activation-testing/07-VALIDATION.md` returns >= 10 (category labels in table)
  </verify>
  <done>
- 3 technical activation prompts tested with keyword overlap analysis, all PASS
- 3 conversational activation prompts tested with keyword overlap analysis, all PASS
- 4 negative activation prompts tested with keyword non-overlap analysis, all PASS
- 11 reference navigation tests verified with grep execution, all PASS
- 102 grep patterns regression-tested, all return exactly 1 line
- 4 MCP boundary tests documented with content presence/absence evidence, all correctly categorized (2 MCP-appropriate, 2 skill-appropriate)
- Any mechanical failures found are fixed inline and re-verified
- Any behavioral concerns documented in "Notes for Review" for checkpoint
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Activation, navigation, and MCP validation tests for the Astro/Cloudflare skill. Tests verify keyword overlap for auto-activation (3 technical + 3 conversational prompts), grep pattern accuracy for reference navigation, and MCP boundary logic (content coverage verification).</what-built>
  <how-to-verify>
1. Open `.planning/phases/07-activation-testing/07-VALIDATION.md`
2. Review Test 1 (Activation): Check that technical prompts (T1-T3) are realistic developer queries, and conversational prompts (C1-C3) are natural language questions. Check that negative prompts test interesting boundaries (not trivially unrelated)
3. Review Test 2 (Navigation): Check that all 11 reference files are covered, and grep patterns actually target the right sections
4. Review Test 3 (MCP): Check that the boundary between skill and MCP usage is correctly identified via content presence/absence evidence. Note: behavioral MCP testing (actual tool calls) happens in Plan 07-02's session resilience test
5. Check the "Fixes Applied" section (if any) -- do the fixes make sense?
6. Check the "Notes for Review" section (if any) -- any behavioral concerns to address?
7. Overall: Does the validation report give confidence that the skill will work in practice?
  </how-to-verify>
  <resume-signal>Type "approved" to proceed to session resilience testing, or describe issues to address</resume-signal>
</task>

</tasks>

<verification>
- 07-VALIDATION.md exists with Test 1, Test 2, Test 3 sections completed
- All test rows have PASS/FAIL results filled in
- No mechanical FAIL results remain unfixed (grep/keyword failures are corrected inline, re-tested, documented)
- 102 grep patterns all verified as returning exactly 1 line
- Test 1 table has explicit Category column with Technical/Conversational/Negative values
- Test 3 documents content coverage verification with note that behavioral MCP testing deferred to 07-02
</verification>

<success_criteria>
- TEST-01 satisfied: 3 technical + 3 conversational prompts with keyword overlap PASS, 4 negative prompts PASS
- TEST-02 satisfied: 11 navigation tests PASS, 102 grep patterns regression PASS
- TEST-03 satisfied: 4 MCP boundary tests correctly categorized (2 MCP-appropriate, 2 skill-appropriate) with grep evidence
- Zero unfixed mechanical failures in the validation report
- User approves the validation report at checkpoint
</success_criteria>

<output>
After completion, create `.planning/phases/07-activation-testing/07-01-SUMMARY.md`
</output>
